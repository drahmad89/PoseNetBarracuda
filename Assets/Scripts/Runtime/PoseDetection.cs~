using Unity.Barracuda;
using UnityEngine;
using UnityEngine.Video;
using UnityEngine.Events;
using System.Collections.Generic;

using UnityEngine.Rendering;
namespace Pose.Detection
{
    /// <summary>
    /// Pose Detection using Unity Barracuda.
    /// 1. Use an existing pose detection model (HRNet, DeepPose etc) and convert it to onnx model.
    /// 2. Render video to a RenderTarget and use it as the input for the model.
    /// 3. Preprocess the render target before passing it to the model. You can use Compute Shaders for this.
    /// 4. Each model requires a specific color space and resolution, you need to convert the render target first.
    /// 5. The `Model` object along with `IWorker` engine classes from Barracuda package will help you with inference.
    /// 6. Convert the output key points with confidence to actual pose and joint values.
    /// 7. Drive a skeleton/gameobjects with joint values.
    /// </summary>
    public class PoseDetection : MonoBehaviour
    {
        [SerializeField] private VideoPlayer videoPlayer;
        [SerializeField] private ComputeShader preprocessingShader;
        
        [Space]
        [SerializeField] private int imageHeight = 256;
        [SerializeField] private int imageWidth = 256;
        const string INPUT_NAME = "input.1";
        const string HEATMAP_NAME = "397";
        const string OFFSET_NAME = "400";

        [Space]
        [SerializeField] private GameObject videoQuad;
        public  NNModel modelAsset;
        IWorker worker;
        #region private
        private int _videoHeight;
        private int _videoWidth;
        private RenderTexture videoTexture;
        private RenderTexture inputTexture;
        
        private const int numKeypoints = 17;
        // Estimated 2D keypoint locations in videoTexture and their associated confidence values
        private float[][] keypointLocations = new float[numKeypoints][];
        
        //Shader Property
        private static readonly int MainTex = Shader.PropertyToID("_MainTex");
        #endregion
        
        private void Start()
        {
            _videoHeight = (int)videoPlayer.GetComponent<VideoPlayer>().height;
            _videoWidth = (int)videoPlayer.GetComponent<VideoPlayer>().width;
            
            // Create a new videoTexture using the current video dimensions
            videoTexture = new RenderTexture(_videoWidth, _videoHeight, 24, RenderTextureFormat.ARGB32);
            videoPlayer.GetComponent<VideoPlayer>().targetTexture = videoTexture;
            
            //Apply Texture to Quad
            videoQuad.gameObject.GetComponent<MeshRenderer>().material.SetTexture(MainTex, videoTexture);
            videoQuad.transform.localScale = new Vector3(_videoWidth, _videoHeight, videoQuad.transform.localScale.z);
            videoQuad.transform.position = new Vector3(0 , 0, 1);
            
            //Move Camera to keep Quad in view
            var mainCamera = Camera.main;
            if (mainCamera !=null)
            {
                mainCamera.transform.position = new Vector3(0, 0, -(_videoWidth / 2));
                mainCamera.GetComponent<Camera>().orthographicSize = _videoHeight / 2;
            }

            //TODO: Create Model from onnx asset and compile it to an object
            var model = ModelLoader.Load(modelAsset);
            //TODO: Add Layers to model

            //TODO: Create Worker Engine
            worker = BarracudaWorkerFactory.CreateWorker(w thBarracudaWorkerFactory.Type.ComputePrecompiled, model);


        }


        // Unity method that is called every tick/frame
        private void Update()
        {
              
            Texture2D processedImage = PreprocessTexture();

            //TODO: Create Tensor 
            var tensor =  TransformInput(processedImage);
            //TODO: Execute Engine
            var inputs = new Dictionary<string, Tensor> {
                        { INPUT_NAME, tensor }
                   };
            worker.Execute(inputs);
            Tensor outputTensor = worker.PeekOutput(INPUT_NAME);
            //TODO: Process Results
             ProcessResults(worker.PeekOutput(HEATMAP_NAME), worker.PeekOutput(OFFSET_NAME));

            //TODO: Draw Skeleton

            //TODO: Clean up tensors and other resources

            Destroy(processedImage);
        }

        private void OnDisable()
        {
            //TODO: Release the inference engine
            //for (var i = 2; i < outputs.Length; i++)
            {
               //outputs[i].Dispose();
            }
            //Release videoTexture
            videoTexture.Release();
        }

        
        #region Additional Methods
        
        private void ProcessResults(Tensor heatmaps, Tensor offsets)
        {
            // Determine the estimated key point locations using the heatmaps and offsets tensors
            
            // Calculate the stride used to scale down the inputImage
            float stride = (imageHeight - 1) / (heatmaps.shape.height - 1);
            stride -= (stride % 8);
            
            int minDimension = Mathf.Min(videoTexture.width, videoTexture.height);
            int maxDimension = Mathf.Max(videoTexture.width, videoTexture.height);
            
            //Recalculate scale for the keypoints
            var scale = (float) minDimension / (float) Mathf.Min(imageWidth, imageHeight);
            var adjustedScale = (float)maxDimension / (float)minDimension;
            
            // Iterate through heatmaps
            for (int k = 0; k < numKeypoints; k++)
            {
                //Find Location of keypoint
                var locationInfo = LocateKeyPoint(heatmaps, offsets, k);
                
                // The (x, y) coordinates contains the confidence value in the current heatmap
                var coords = locationInfo.Item1;
                var offsetVector = locationInfo.Item2;
                var confidenceValue = locationInfo.Item3;
                
                //Calulate X position and Y position
                var xPos = (coords[0]*stride + offsetVector[0])*scale;
                var yPos = (imageHeight - (coords[1]*stride + offsetVector[1]))*scale;
                if (videoTexture.width > videoTexture.height) {
                    xPos *= adjustedScale;
                }
                else
                {
                    yPos *= adjustedScale;
                }
                
                keypointLocations[k] = new float[] { xPos, yPos, confidenceValue };
            }
        }
        
        private (float[],float[],float) LocateKeyPoint(Tensor heatmaps, Tensor offsets, int i)
        {
            //Find the heatmap index that contains the highest confidence value and the associated offset vector
            var maxConfidence = 0f;
            var coords = new float[2];
            var offsetVector = new float[2];
            
            // Iterate through heatmap columns
            for (int y = 0; y < heatmaps.shape.height; y++)
            {
                // Iterate through column rows
                for (int x = 0; x < heatmaps.shape.width; x++)
                {
                    if (heatmaps[0, y, x, i] > maxConfidence)
                    {
                        maxConfidence = heatmaps[0, y, x, i];
                        coords = new float[] { x, y };
                        offsetVector = new float[]
                        {
                            offsets[0, y, x, i + numKeypoints],
                            offsets[0, y, x, i]
                        };
                    }
                }
            }
            return (coords, offsetVector, maxConfidence);
        }

        private Texture2D PreprocessTexture()
        {
            //Apply any kind of preprocessing if required - Resize, Color values scaled etc
            
            Texture2D imageTexture = new Texture2D(videoTexture.width, 
                videoTexture.height, TextureFormat.RGBA32, false);
            
            Graphics.CopyTexture(videoTexture, imageTexture);
            Texture2D tempTex = Resize(imageTexture, imageHeight, imageWidth);
            Destroy(imageTexture);

            // TODO: Apply model-specific preprocessing
            imageTexture = PreprocessNetwork(tempTex);
            
            Destroy(tempTex);
            return imageTexture;
        }

        private Texture2D PreprocessNetwork(Texture2D inputImage)
        {
            // Use Compute Shaders (GPU) to preprocess your image
            // Each model requires a specific color space - RGB 
            // Values need to scaled to what it was trained on
            
            var numthreads = 8;
            var kernelHandle = preprocessingShader.FindKernel("Preprocess");
            var rTex = new RenderTexture(inputImage.width, 
                inputImage.height, 0, RenderTextureFormat.ARGBFloat);//depth buffer to 0 and the onnx take float32 input
            rTex.enableRandomWrite = true;
            rTex.Create();
            
            preprocessingShader.SetTexture(kernelHandle, "Result", rTex);
            preprocessingShader.SetTexture(kernelHandle, "InputImage", inputImage);
            preprocessingShader.Dispatch(kernelHandle, inputImage.height
                                                       / numthreads, 
                inputImage.width / numthreads, 1);
            
            RenderTexture.active = rTex;
            Texture2D nTex = new Texture2D(rTex.width, rTex.height, TextureFormat.RGBAHalf, false);
            Graphics.CopyTexture(rTex, nTex);
            RenderTexture.active = null;
            Destroy(rTex);
            //AsyncGPUReadback.Request(renderTexture, 0, TextureFormat.RGB24, OnCompleteReadback);

            return nTex;
        }
        void OnCompleteReadback(AsyncGPUReadbackRequest request)
        {

            if (request.hasError)
            {
                Debug.Log("GPU readback error detected.");
                return;
            }

            //callback.Invoke(request.GetData<byte>().ToArray());
        }
    
        private Texture2D Resize(Texture2D image, int newWidth, int newHeight)
            {
                RenderTexture rTex = RenderTexture.GetTemporary(newWidth, newHeight, 24);

                RenderTexture.active = rTex;
            
                Graphics.Blit(image, rTex);

                Texture2D nTex = new Texture2D(newWidth, newHeight, TextureFormat.RGBA32, false);
            
                Graphics.CopyTexture(rTex, nTex);

                RenderTexture.active = null;
            
                RenderTexture.ReleaseTemporary(rTex);
                return nTex;
            }
        private Tensor TransformInput(Texture2D inputImage)
            {
                byte[] pixels = inputImage.EncodeToPNG();

                float[] transformedPixels = new float[pixels.Length];

                for (int i = 0; i < pixels.Length; i++)
                {
                    transformedPixels[i] = pixels[i] / 256f;
                }
                
                return new Tensor(1, inputImage.height, inputImage.width, 3, transformedPixels);
            }
            #endregion
    }
}